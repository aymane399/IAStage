{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we're going to prepare the feature and target vectors for two scores, and then we'll do the grouping lists that we are gonna use for cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ayman\\appdata\\local\\programs\\python\\python36\\lib\\importlib\\_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n",
      "c:\\users\\ayman\\appdata\\local\\programs\\python\\python36\\lib\\importlib\\_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n",
      "c:\\users\\ayman\\appdata\\local\\programs\\python\\python36\\lib\\importlib\\_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import nilearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob as gl\n",
    "import os\n",
    "%matplotlib inline\n",
    "\n",
    "from nilearn.plotting import plot_connectome\n",
    "from nilearn.connectome import ConnectivityMeasure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set of scores:\n",
    "Gold_data = pd.read_table('phenotype/GoldMSI.tsv').dropna()\n",
    "\n",
    "#lists of retained subjects:\n",
    "\n",
    "subjects_list =[i[-3:] for i in list(Gold_data['participant_id'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the groups lists we'll need the following function that returns the session number given the filename and the two dictionnaries that bind subject numbers to their target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[19.0, 7.0]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(Gold_data.iloc[0,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Session_from_name(name):\n",
    "    param1 = name[90:92]\n",
    "    param2 = name[97:99]\n",
    "    if param1 == 'AP' and param2 == '01':\n",
    "        return '1'\n",
    "    elif param1 == 'AP' and param2 == '02':\n",
    "        return '2'\n",
    "    elif param1 == 'PA' and param2 == '01':\n",
    "        return '3'\n",
    "    else:\n",
    "        return '4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_dict ={}\n",
    "\n",
    "for i in range(len(subjects_list)):\n",
    "    Y_dict[subjects_list[i]] = list(Gold_data.iloc[i,1:])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll have to scoure the fMRI dataset files to find those associated to subjects we're interested, we'll create the time series necessary for the tangent vectors' calculation, and at the same time we'll fill the groups and target lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=[]\n",
    "group_bysubject=[]\n",
    "time_series = []\n",
    "for subject_num in subjects_list:\n",
    "    for subject_file in gl.glob('allsubjs/subj_010'+subject_num+'*'):\n",
    "        time_series.append(np.load(subject_file)['ts'])\n",
    "        group_bysubject.append(subject_num)\n",
    "        Y.append(Y_dict[subject_num])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is creating the tangent vectors with the nilearn method ConnectivityMeasure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tangent_measure = ConnectivityMeasure(kind='tangent',vectorize=True,discard_diagonal=True)\n",
    "tangent_vectors = tangent_measure.fit_transform(time_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll save all the arrays for future usage/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('data.npz',X=tangent_vectors, Y=Y, groups_bysubject = group_bysubject)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
